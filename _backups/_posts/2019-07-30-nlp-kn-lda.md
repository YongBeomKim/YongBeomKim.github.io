---
title : 자연어 강의 5일 - LDA
last_modified_at: 2019-07-30T10:45:06-05:00
header:
  overlay_image: /assets/images/book/nlp_master.jpg
categories:
  - nlp
tags: 
    - nlp
    - python
---

**Page Link** : Document 별 수집경로의 가중치를 정의한 뒤, 수집된 문서별 **가중치 값을** 저장 합니다.
**DTM , TMD 데이터를 저장하는** 경우 데이터 베이스에 저장을 합니다. **TDM 모델을** 만든 뒤 문서의 Token 별로 인덱스를 풀어낼 수 있도록 저장합니다. 자세한 내용은 [NDBM Tutorial](https://franz.com/support/tutorials/ndbm-tutorial.htm) 내용을 참조 합니다

Question!!
1. **TDM** 을 DataBase 에 저장하면 새로운 문서가 나올때 마다 새로저장??
1. **TWM** 이란? **DVL** 이란? [참신러닝](https://leechamin.tistory.com/141)
2. 우분투 에서 **Mecab** 말고 추가로 가능한 것은?

<br/>
# 확률의 계산

개별확률과 결합확률을 알면, 조건부 확률을 계산할 수 있습니다.

$$ P(X|Y) = P(X,Y) / P(Y) $$



<br/>
# Latent Dirichlet Allocation

Topic 모델링 방법으로 대표적이다

문서의 분포만으로 주제를 잠재적으로 추정 가능한 모델링 입니다.

<figure class="align-center">
  <img src="https://www.mathworks.com/help/examples/textanalytics/win64/FitLDAModelToWordCountMatrixExample_01.png">
  <figcaption></figcaption>
</figure>

LDA 분석을 위해선 개별 Topic 별로 word 를 특정 가능합니다

<figure class="align-center">
  <img src="http://nlpx.net/wp/wp-content/uploads/2016/01/LDA_image2.jpg">
  <figcaption></figcaption>
</figure>

## Topic Modeling

개별 특징들을 찾아내는 방법
1. Topic 의 갯수
2. The Proportion of Topics  
3. The Most probable words in topics (주제별 중요 단어들)
4. Text analysis without reading the whold corpus 

현황에 대한 분석 

해당 내용에 대한 생각

문제점 지적

해결책 제시 (기승전결)

## LDA 모델링

Theta D 번쨰 : Topic

Z : 개별 단어들의 Topic 해당하는 확률

<figure class="align-center">
  <img src="https://miro.medium.com/max/882/1*pZo_IcxW1GVuH2vQKdoIMQ.jpeg">
  <figcaption></figcaption>
</figure>

## Dirichlet Distribution

**디리클레 분포** 를 사용하여 Alpha 와 Beta 값은 고정 됩니다.

\theta_i (문서의 Topic 분포) ~ Dir(\alpha), i 

\pi_k (Topic 단어의 분포 : 조건부 확률) ~ Dir(\beta), k

cf) Pi 값은 Topic 갯수만큼 존재합니다.

Z_{i,z} ~ i (문서) l(token의 번호)  : Pi 에서 단어를 가져올 수 있다

W_{i,z} ~ Multi(pi_{z{i,j}}) (단어) : 유일하게 측정 가능한 값


## Gibbs Sampling

Marginalization, Summing Out

나머지들은 무시하고 샘플링하는 방법으로 Collapsed Gibbs Sampling (Grapical Sampling)

콜렙스트 깁스샘플링 방법입니다.

lda 방식에서는 1개의 단어 수집을 위해서는, 주변의 단어들을 활용합니다. 

<figure class="align-center">
  <img src="https://i.stack.imgur.com/EuSz5.png">
  <figcaption></figcaption>
</figure>

Prior Bayse 모델로 개별 단어들이 Topic 해당여부를 Given 으로 모델을 해석합니다. (조건부 확률을 사용하여 찾습니다.)

ㅠ^M_j   ㅠ^N_l P(W_{i,l}|Z_{j,l})

$$ n^{i}_{j,i,k} = 2 $$

**j 문서** 의, **i 토픽** 의, **k 단어 인덱스** 로 해당 단어별 수식을 정리할 수 있습니다.

<figure class="align-center">
  <img src="https://i.stack.imgur.com/DujEH.png">
  <figcaption> 확률의 합을 통해 뒷부분을 1로 정리를 합니다</figcaption>
</figure>

문서마다 갖고있는 Topic 의 갯수 확률과, 단어의 빈도값을 활용하여 **디리클레 분포** 에서 **Z** 값을 계산하는 공식으로 정리를 합니다.

**Conjugate Prior** 과정을(모든 발생확률을 더해서 1로 변환) 통해, Z 이외의 변수값들을 모두 제거하였습니다.

Z 값은 **Token 별로** 계산되는 값으로 주변 단어들에 의해 모델링의 영향을 받습니다.

$$\Gamma(x) = (x-1)!$$

## Parameter Inference

**perplexity :** LDA 의 Alpha 와 Beta 값이 클수록 중심으로 모인다, 따라서 Alpha 와 Beta 값이 작아서 계산된 Z 값들이 각각의 Topic 별로 구분이 가능하도록 반복학습을 진행 합니다.

이를 반복하면 결과값은 다음의 2가지 결과를 도출할 수 있습니다.

1. $\theta$ 값은 **문서** 마다 Topic 의 분포 분석결과 
2. $\pi$ 값은 **단어** 마다 Topic 의 분포 분석결과 (WordCloud 상 빈도가 높은 단어)


$$A^2 + B^2 = C^2$$

Question!
Q. 그럼 Topic 단어들은 미리 수집되어 있어야 하나?

{% raw %}
$$a^2 + b^2 = c^2$$
{% endraw %}

## RDBMS , NDBM

<br/>
# 감성분석

트위터를 활용한 감성분석 예제 [논문보기](http://etf-central.com/2010/10/21/update-j-bollen-h-mao-x-j-zeng-twitter-mood-predicts-the-stock-market/)

<figure class="align-center">
  <img src="http://etf-central.com/wp-content/uploads/2010/10/20101021_twitter_zscore_2.jpg">
  <figcaption></figcaption>
</figure>

PMI (Point wised Mutual Information) : 통계를 사용한 방법으로 point Wise 한 점을 찍어서 유사한 그룹들ㅇ르 찾습니다.

Simentic Orientation (SO) : So(x) = PMI(Positive) - PMI(Negative)